{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4467d7-7d03-44dc-bd4e-07a83e0a2ff4",
   "metadata": {},
   "source": [
    "## **The Training Data**\n",
    "<p>The Landslide4Sense data consists of the training, validation, and test sets containing 3799, 245, and 800 image patches, respectively. Each image patch is a composite of 14 bands that include:\n",
    "\n",
    "> **Multispectral data from Sentinel-2 : B1, B2, B3, B4, B5, B6, B7, B8, B9, B10, B11, B12**.\n",
    ">\n",
    "> **Slope data from ALOS PALSAR: B13**.\n",
    ">\n",
    "> **Digital elevation model (DEM) from ALOS PALSAR: B14**.\n",
    "\n",
    "All bands in the competition dataset are resized to the resolution of ~10m per pixel. The image patches have the size of 128 x 128 pixels and are labeled pixel-wise.</p>\n",
    "\n",
    "## **The Testing Data**\n",
    "<p>Including UAV data of Van Yen and Mu Cang Chai districts:\n",
    "    \n",
    "> **High resolution ~ 0.17m**.\n",
    ">\n",
    "> **3 bands - Red, Green, Blue**.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770b208-d701-4e0a-8f7f-c995b69d4a73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Pre-processing**\n",
    "<p>\n",
    "    This step is for data preparation and tuning. Particularly, data will be changed on how data are stored but remain the original size of (128,128,3). Data can be flipped, rotated, clipped or so on to increase the sense of model and support predictions with more details of recognitivity\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4016bfa0-d51b-4bac-a1f7-fdc8a3fa1417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOCAL_TRAINDATA_DIR=Data/TrainData\n",
      "env: LOCAL_VALIDDATA_DIR=Data/ValidData\n",
      "env: LOCAL_PARENT_DIR=Input\n",
      "env: LOCAL_PREDICT_SET_DIR=Predict_Set\n"
     ]
    }
   ],
   "source": [
    "# set environment variables\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "%set_env LOCAL_TRAINDATA_DIR=Data/TrainData\n",
    "%set_env LOCAL_VALIDDATA_DIR=Data/ValidData\n",
    "%set_env LOCAL_PARENT_DIR = Input\n",
    "%set_env LOCAL_PREDICT_SET_DIR = Predict_Set\n",
    "\n",
    "# set paths for training images, masks, and valid images\n",
    "image_dir=os.path.join(os.getenv('LOCAL_TRAINDATA_DIR'), 'img')\n",
    "mask_dir=os.path.join(os.getenv('LOCAL_TRAINDATA_DIR'), 'mask')\n",
    "val_dir = os.path.join(os.getenv('LOCAL_VALIDDATA_DIR'), 'img')\n",
    "pred_dir = os.path.join(os.getenv('LOCAL_PREDICT_SET_DIR'),'')\n",
    "parent_dir = os.path.join(os.getenv('LOCAL_PARENT_DIR'), '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a84ea-b326-42d5-a046-cfd31a27db3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **#1 - Finding data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd781f2-67ef-4c7f-9c7d-e1e5019a5a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are found in folder Data/TrainData\\img in 6.886787176132202 seconds\n",
      "Files are found in folder Data/TrainData\\mask in 16.099828720092773 seconds\n",
      "Extracting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/3799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert h5py to numpy array done in 2.25927472114563 seconds\n",
      "Files are found in folder Data/ValidData\\img in 1.3205757141113281 seconds\n"
     ]
    }
   ],
   "source": [
    "image_files, image_holder = pp.find_data(image_dir) # find training images\n",
    "\n",
    "label_files, label_holder = pp.find_data(mask_dir) # find training labels\n",
    "mask_holder = pp.h5label_to_array(label_holder) # extracting label array\n",
    "\n",
    "val_files, val_holder = pp.find_data(val_dir) # find validating images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99c62a-6ed6-49df-a75b-bfa777d25890",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **#2 - Nomarlize data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e87841-2f4c-4659-b694-7d8cd97f4bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start normalization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalization:   0%|          | 0/3799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel-goes-last (height, width, channel)\n",
      "Free RAM space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading...:   0%|          | 0/3799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization is processed in 243.88164925575256 seconds\n",
      "Start normalization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalization:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel-goes-last (height, width, channel)\n",
      "Free RAM space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading...:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization is processed in 15.383058786392212 seconds\n"
     ]
    }
   ],
   "source": [
    "image_holder_norms = pp.rgb_norm_data(image_holder, data_type='float32')\n",
    "val_holder_norms = pp.rgb_norm_data(val_holder, data_type='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810237c8-1749-4c22-9f42-3212efb9a04f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **#3 - Data Augmentation for Training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38d57fa-e334-43ed-a0f7-a4c26428ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "In Flipping loop:   0%|          | 0/3799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping done in 2.0563392639160156 seconds\n",
      "Rotating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "In Rotating loop:   0%|          | 0/3799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotating done in 109.38085913658142 seconds\n",
      "Padding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "In Padding loop:   0%|          | 0/3799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "In Trimming loop:   0%|          | 0/3799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming done in 15.271093845367432 seconds\n"
     ]
    }
   ],
   "source": [
    "# flip data\n",
    "flipped_images, flipped_masks = pp.flip_image(image_holder_norms, label_holder, axis_to_flip=None)\n",
    "# rotate data 30 degree\n",
    "rotated_images, rotated_masks = pp.rotate_image(image_holder_norms, label_holder, 30, reshape=False)\n",
    "# trim data\n",
    "trim_images, trim_masks = pp.trim_array(image_holder_norms, mask_holder, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a309c-0d43-44f3-9c5c-4288a8b4dfdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **#4 - Save to Numpy file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324a20d4-ea37-4a14-b0cf-3aefd2d7204d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy files are created in 57.7584433555603 seconds\n"
     ]
    }
   ],
   "source": [
    "# save file to npy\n",
    "import os\n",
    "os.chdir(os.getenv('LOCAL_PARENT_DIR')) # change directory\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "\n",
    "# stack\n",
    "image_stack = np.stack(image_holder_norms + flipped_images + rotated_images + trim_images) # concatenate the image_holder_norms, flipped_images, rotated_images, trim_images\n",
    "label_stack = np.stack(mask_holder + flipped_masks + rotated_masks + trim_masks) # concatenate the mask_holder, flippped_masks, rotated_masks, trim_masks\n",
    "valid_stack = np.stack(val_holder_norms) # stack val_holder_norms\n",
    "\n",
    "# save now\n",
    "np.save('image_patch128.npy', image_stack)\n",
    "np.save('mask_patch128.npy', label_stack)\n",
    "np.save('valid_patch128.npy', valid_stack)\n",
    "\n",
    "final_time = time.time()\n",
    "tot_time = final_time - init_time\n",
    "#----------------------------------------------------------------\n",
    "print(f'Numpy files are created in {tot_time} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
